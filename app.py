import json
import time
from pathlib import Path

import streamlit as st
import pandas as pd

import pipeline
import indexation

# Constantes
DATA_DIR = Path('wiki_split_extract_2k')
PICKLE_PATH = Path('preprocessed_data.pkl')
TFIDF_PATH = Path('tfidf_model.pkl')
RESULTS_PATH = Path('evaluation_results.json')
QUERIES_PATH = Path('requetes.jsonl')
DEFAULT_KS = [1, 5, 10]


@st.cache_resource(show_spinner=False)
def get_cached_documents(force_preprocess):
    """Charge les documents sans appel √† st.toast."""
    return pipeline.load_or_preprocess_documents(
        directory_path=str(DATA_DIR),
        output_file=str(PICKLE_PATH),
        force_preprocess=force_preprocess,
        log_fn=lambda msg: None,  # Ne rien faire pour √©viter les probl√®mes de cache
    )


@st.cache_resource(show_spinner=False)
def get_cached_tfidf(_documents, force_tfidf):
    """Charge le TF-IDF sans appel √† st.toast."""
    return pipeline.load_or_compute_tfidf(
        _documents,
        model_file=str(TFIDF_PATH),
        force_tfidf=force_tfidf,
        log_fn=lambda msg: None,  # Ne rien faire pour √©viter les probl√®mes de cache
    )


def run_full_pipeline(k, top_k, rerank, force_preprocess, force_tfidf):
    class Args:
        pass

    args = Args()
    args.k = k
    args.top_k = top_k
    args.rerank = rerank
    args.force_preprocess = force_preprocess
    args.force_tfidf = force_tfidf

    logs = []

    def log_fn(msg):
        logs.append(msg)

    output = pipeline.run_pipeline(args, log_fn=log_fn)

    return output, logs


def search_query(query, documents, X, vectorizer, k, rerank, top_k_for_reranking):
    """Effectue une recherche pour une requ√™te unique."""
    if rerank:
        top_tfidf = indexation.get_top_k_documents(X, vectorizer, query, documents, top_k_for_reranking)
        results = indexation.rerank_with_cross_encoder(query, top_tfidf, documents)
        return results[:k]
    else:
        return indexation.get_top_k_documents(X, vectorizer, query, documents, k)


def main():
    st.set_page_config(page_title='IR Pipeline Streamlit', layout='wide')
    st.title('üîç Interface Streamlit ‚Äì Pipeline TF-IDF + Cross-Encoder')

    with st.sidebar:
        st.header('‚öôÔ∏è Configuration')

        st.subheader('Param√®tres de recherche')
        k = st.slider('Nombre de r√©sultats finaux (k)', min_value=1, max_value=50, value=10)
        top_k = st.slider('Top-k pour reranking', min_value=k, max_value=150, value=max(30, k))
        rerank = st.checkbox('Activer le reranking cross-encoder', value=False)

        st.subheader('Options avanc√©es')
        force_preprocess = st.checkbox('Forcer le pr√©traitement', value=False)
        force_tfidf = st.checkbox('Forcer le recalcul TF-IDF', value=False)

        st.divider()
        st.info('üí° Configurez les options puis utilisez les onglets ci-dessus.')

    tab1, tab2 = st.tabs(['üîé Recherche Interactive', 'üìä √âvaluation Compl√®te'])

    with tab1:
        st.header('Recherche de documents')
        st.write('Entrez une requ√™te pour rechercher des documents pertinents dans la collection.')

        query = st.text_input('üîç Votre requ√™te:', placeholder='Ex: qu\'est-ce que la 6e arm√©e')

        col1, col2 = st.columns([1, 4])
        with col1:
            search_button = st.button('üöÄ Rechercher', type='primary', use_container_width=True)

        if search_button and query:
            with st.spinner('Chargement des donn√©es...'):
                documents = get_cached_documents(force_preprocess)
                X, vectorizer = get_cached_tfidf(documents, force_tfidf)

            with st.spinner('Recherche en cours...'):
                start_time = time.time()
                results = search_query(query, documents, X, vectorizer, k, rerank, top_k)
                search_time = time.time() - start_time

            st.success(f'‚úÖ Recherche termin√©e en {search_time:.2f}s')

            st.subheader(f'Top {len(results)} r√©sultats pour: "{query}"')

            if results:
                df_results = pd.DataFrame([
                    {
                        'Rang': i + 1,
                        'Document ID': doc_id,
                        'Score': f'{score:.4f}'
                    }
                    for i, (doc_id, score) in enumerate(results)
                ])

                st.dataframe(df_results, use_container_width=True, hide_index=True)

                st.subheader('üìÑ D√©tails des documents')
                for i, (doc_id, score) in enumerate(results[:5]):
                    with st.expander(f'#{i+1} - {doc_id} (Score: {score:.4f})'):
                        doc = next((d for d in documents if d['doc_id'] == doc_id), None)
                        if doc:
                            tokens = doc['tokens']
                            st.write(f'**Nombre de tokens:** {len(tokens)}')
                            st.write(f'**Extrait (100 premiers tokens):**')
                            st.text(' '.join(tokens[:100]) + '...')
                        else:
                            st.warning('Document non trouv√© dans le corpus.')
            else:
                st.warning('Aucun r√©sultat trouv√©.')

        elif search_button and not query:
            st.warning('‚ö†Ô∏è Veuillez entrer une requ√™te.')

    with tab2:
        st.header('√âvaluation du pipeline complet')
        st.write('Lancez l\'√©valuation sur toutes les requ√™tes du fichier `requetes.jsonl`.')

        run_button = st.button('‚ñ∂Ô∏è Lancer le pipeline d\'√©valuation', type='primary')

        if run_button:
            with st.spinner('Ex√©cution du pipeline...'):
                start = time.time()
                output, logs = run_full_pipeline(k, top_k, rerank, force_preprocess, force_tfidf)
                duration = time.time() - start

            st.success(f'‚úÖ Pipeline termin√© en {duration:.1f}s')

            with st.expander('üìã Journal d\'ex√©cution'):
                st.code('\n'.join(logs) or 'Aucun log')

            st.subheader('üìä M√©triques d\'√©valuation')

            metrics = output['metrics']

            col1, col2, col3, col4 = st.columns(4)

            with col1:
                st.metric('MRR', f"{metrics['MRR']:.4f}")
            with col2:
                st.metric('MAP', f"{metrics['MAP']:.4f}")
            with col3:
                st.metric('P@1', f"{metrics['P@k'][1]:.4f}")
            with col4:
                st.metric('P@5', f"{metrics['P@k'][5]:.4f}")

            st.subheader('Pr√©cision et Rappel par k')
            metrics_df = pd.DataFrame({
                'k': list(metrics['P@k'].keys()),
                'Pr√©cision@k': [f"{v:.4f}" for v in metrics['P@k'].values()],
                'Rappel@k': [f"{v:.4f}" for v in metrics['R@k'].values()]
            })
            st.dataframe(metrics_df, use_container_width=True, hide_index=True)

            with st.expander('üìë R√©sultats d√©taill√©s par requ√™te'):
                for qid, docs in output['retrieved'].items():
                    st.write(f'**{qid}**')
                    st.write(', '.join(docs[:10]))

            with st.expander('üìÑ M√©triques compl√®tes (JSON)'):
                st.json(metrics)

            st.download_button(
                label='‚¨áÔ∏è T√©l√©charger les r√©sultats JSON',
                data=json.dumps(metrics, ensure_ascii=False, indent=2),
                file_name='evaluation_results.json',
                mime='application/json'
            )


if __name__ == '__main__':
    main()
